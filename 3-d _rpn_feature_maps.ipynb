{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers as KL\n",
    "import keras.engine as KE\n",
    "import keras.models as KM\n",
    "import config\n",
    "config=config.Config()\n",
    "# cfg=tf.ConfigProto()\n",
    "# cfg.gpu_options.per_process_gpu_memory_fraction=0.4\n",
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pyramid Network for 3d object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d- resnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(KL.BatchNormalization):\n",
    "    \"\"\"Extends the Keras BatchNormalization class to allow a central place\n",
    "    to make changes if needed.\n",
    "    Batch normalization has a negative effect on training if batches are small\n",
    "    so this layer is often frozen (via setting in Config class) and functions\n",
    "    as linear layer.\n",
    "    \"\"\"\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Note about training values:\n",
    "            None: Train BN layers. This is the normal mode\n",
    "            False: Freeze BN layers. Good when batch size is small\n",
    "            True: (don't use). Set layer in training mode even when making inferences\n",
    "        \"\"\"\n",
    "        return super(self.__class__, self).call(inputs, training=training)\n",
    "    \n",
    "###identity_block for 3d-resnet\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block,\n",
    "                   use_bias=True, train_bn=True):\n",
    "    \"\"\"The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        use_bias: Boolean. To use or not use a bias in conv layers.\n",
    "        train_bn: Boolean. Train or freeze Batch Norm layers\n",
    "    \"\"\"\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = KL.Conv3D(nb_filter1, (1, 1, 1), name=conv_name_base + '2a',\n",
    "                  use_bias=use_bias)(input_tensor)\n",
    "    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv3D(nb_filter2, (kernel_size, kernel_size, kernel_size), padding='same',\n",
    "                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv3D(nb_filter3, (1, 1, 1), name=conv_name_base + '2c',\n",
    "                  use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n",
    "\n",
    "    x = KL.Add()([x, input_tensor])\n",
    "    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###conv_block for 3d-resnet\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block,\n",
    "               strides=(2, 2, 2), use_bias=True, train_bn=True):\n",
    "    \"\"\"conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        use_bias: Boolean. To use or not use a bias in conv layers.\n",
    "        train_bn: Boolean. Train or freeze Batch Norm layers\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    \"\"\"\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = KL.Conv3D(nb_filter1, (1, 1, 1), strides=strides,\n",
    "                  name=conv_name_base + '2a', use_bias=use_bias)(input_tensor)\n",
    "    x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv3D(nb_filter2, (kernel_size, kernel_size, kernel_size), padding='same',\n",
    "                  name=conv_name_base + '2b', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "\n",
    "    x = KL.Conv3D(nb_filter3, (1, 1, 1), name=conv_name_base +\n",
    "                  '2c', use_bias=use_bias)(x)\n",
    "    x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn)\n",
    "\n",
    "    shortcut = KL.Conv3D(nb_filter3, (1, 1, 1), strides=strides,\n",
    "                         name=conv_name_base + '1', use_bias=use_bias)(input_tensor)\n",
    "    shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn)\n",
    "\n",
    "    x = KL.Add()([x, shortcut])\n",
    "    x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bottom to up Layers for FPN using 3d-resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete restnet block \n",
    "def resnet_graph(input_image, architecture, stage5=False, train_bn=True):\n",
    "    \"\"\"Build a ResNet graph.\n",
    "        architecture: Can be resnet50 or resnet101\n",
    "        stage5: Boolean. If False, stage5 of the network is not created\n",
    "        train_bn: Boolean. Train or freeze Batch Norm layers\n",
    "    \"\"\"\n",
    "    assert architecture in [\"resnet50\", \"resnet101\"]\n",
    "    # Stage 1\n",
    "    x = KL.ZeroPadding3D((3, 3, 3), data_format='channels_last')(input_image)\n",
    "    x = KL.Conv3D(64, (7, 7, 7), strides=(2, 2, 2), name='conv1', use_bias=True)(x)\n",
    "    x = BatchNorm(name='bn_conv1')(x, training=train_bn)\n",
    "    x = KL.Activation('relu')(x)\n",
    "    C1 = x = KL.MaxPooling3D((3, 3, 3), strides=(2, 2, 2), padding=\"same\")(x)\n",
    "    # Stage 2\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1, 1), train_bn=train_bn)\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn)\n",
    "    C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn)\n",
    "    # Stage 3\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn)\n",
    "    C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn)\n",
    "    # Stage 4\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn)\n",
    "    block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]\n",
    "    for i in range(block_count):\n",
    "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn)\n",
    "    C4 = x\n",
    "    # Stage 5\n",
    "    if stage5:\n",
    "        x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn)\n",
    "        x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn)\n",
    "        C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn)\n",
    "    else:\n",
    "        C5 = None\n",
    "    return [C1, C2, C3, C4, C5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Test model for Tests/Validations\n",
    "def resnet_test_model():\n",
    "    input_image=KL.Input(\n",
    "        shape=[None, None, None,1], name=\"input_image2\")\n",
    "    x = resnet_graph(input_image,config.BACKBONE,\n",
    "                                         stage5=True, train_bn=config.TRAIN_BN)\n",
    "    model= KM.Model(inputs=input_image, outputs=x) \n",
    "   # model.summary()\n",
    "\n",
    "    return  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=resnet_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model in image\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file=\"3d_resnet_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=tf.random_uniform((1,512,2048,1024,1))\n",
    "C1, C2, C3, C4, C5=model([img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_1/res3d_out/Relu:0' shape=(1, 64, 256, 128, 512) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_1/res2c_out/Relu:0' shape=(1, 128, 512, 256, 256) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-down Layers for FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-down Layers\n",
    "# TODO: add assert to varify feature map sizes match what's in config\n",
    "\n",
    "P5 = KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1, 1), name='fpn_c5p5')(C5)\n",
    "P4 = KL.Add(name=\"fpn_p4add\")([\n",
    "    KL.UpSampling3D(size=(2, 2, 2), name=\"fpn_p5upsampled\")(P5),\n",
    "    KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1, 1), name='fpn_c4p4')(C4)])\n",
    "P3 = KL.Add(name=\"fpn_p3add\")([\n",
    "    KL.UpSampling3D(size=(2, 2, 2), name=\"fpn_p4upsampled\")(P4),\n",
    "    KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1, 1), name='fpn_c3p3')(C3)])\n",
    "P2 = KL.Add(name=\"fpn_p2add\")([\n",
    "    KL.UpSampling3D(size=(2, 2, 2), name=\"fpn_p3upsampled\")(P3),\n",
    "    KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1, 1), name='fpn_c2p2')(C2)])\n",
    "# Attach 3x3 conv to all P layers to get the final feature maps.\n",
    "P2 = KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n",
    "P3 = KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n",
    "P4 = KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n",
    "P5 = KL.Conv3D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n",
    "# P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "# subsampling from P5 with stride of 2.\n",
    "P6 = KL.MaxPooling3D(pool_size=(1, 1, 1), strides=2, name=\"fpn_p6\")(P5)\n",
    "\n",
    "# Note that P6 is used in RPN, but not in the classifier heads.\n",
    "rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
    "mrcnn_feature_maps = [P2, P3, P4, P5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'fpn_p2/add:0' shape=(1, 128, 512, 256, 256) dtype=float32>,\n",
       " <tf.Tensor 'fpn_p3/add:0' shape=(1, 64, 256, 128, 256) dtype=float32>,\n",
       " <tf.Tensor 'fpn_p4/add:0' shape=(1, 32, 128, 64, 256) dtype=float32>,\n",
       " <tf.Tensor 'fpn_p5/add:0' shape=(1, 16, 64, 32, 256) dtype=float32>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can directly compare shapes eith the corresponding 3D version\n",
    "mrcnn_feature_maps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_1/res3d_out/Relu:0' shape=(1, 64, 256, 128, 512) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A hack for saving current loaded graph, so that it can be viewed later on tensorboard \n",
    "writer = tf.summary.FileWriter(\"/logs/3d_graph\", sess.graph)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
